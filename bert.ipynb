{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import convert_bert_original_tf_checkpoint_to_pytorch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForQuestionAnswering, BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'bert_model.ckpt.data-00000-of-00001',\n",
       " 'bert_config.json',\n",
       " 'vocab.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'bert_model.ckpt.index']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERT_MODEL_PATH = \"rubert/\"\n",
    "os.listdir(BERT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch model from configuration: {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_bert:Converting TensorFlow checkpoint from /Users/potemin/work/rustory_generation/rubert/bert_model.ckpt\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/word_embeddings with shape [119547, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/output_bias with shape [119547]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "INFO:transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
      "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save PyTorch model to pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "convert_bert_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
    "BERT_MODEL_PATH + 'bert_config.json','pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"rubert_torch/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:Model name 'rubert_torch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'rubert_torch/' is a path or url to a directory containing tokenizer files.\n",
      "INFO:transformers.tokenization_utils:Didn't find file rubert_torch/added_tokens.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:Didn't find file rubert_torch/special_tokens_map.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:Didn't find file rubert_torch/tokenizer_config.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:loading file rubert_torch/vocab.txt\n",
      "INFO:transformers.tokenization_utils:loading file None\n",
      "INFO:transformers.tokenization_utils:loading file None\n",
      "INFO:transformers.tokenization_utils:loading file None\n",
      "INFO:transformers.configuration_utils:loading configuration file rubert_torch/config.json\n",
      "INFO:transformers.configuration_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file rubert_torch/pytorch_model.bin\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=False)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
