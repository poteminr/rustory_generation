{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtokentome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/17/72cb4f7e01941e663e560d7d1882bcfa9794af917e09c9c319837d083138/youtokentome-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 2.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.6/site-packages (from youtokentome) (7.0)\n",
      "Installing collected packages: youtokentome\n",
      "Successfully installed youtokentome-1.0.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install youtokentome\n",
    "# !pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HzETJPTeDI2l"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import youtokentome as yttm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaS0OXNbDI2v"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 17 13:11:04 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   39C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4swSGSVDI25"
   },
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTSxoD8cDI25"
   },
   "outputs": [],
   "source": [
    "story_path = \"corpus2/story_data_punct_del_em.pkl\"\n",
    "with open(story_path, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iNrocaqPSifM"
   },
   "outputs": [],
   "source": [
    "STORY_VAL = 8500\n",
    "data_batch = data[:STORY_VAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples2text(data):\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        if len(data[i]) < 200:\n",
    "            new_data.append(\" \".join(data[i]).replace(\" .\",\".\").replace(\" ,\", \",\").replace(\" ?\", \"?\"))\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch = list(samples2text(data_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'п 19, встречаюсь с армянкой 21, так вот, я парень хоть куда и клялся влюбви и женитьбе, она в итоге мне дала, но я все время перся со всеми подряд, и сейчас не беру от неё трубки за левобережку меня'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_story = np.random.randint(0, len(data_batch)-1)\n",
    "data_batch[random_story]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning size: 5892 | Validating size: 1965\n",
      "Total samples:7857\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = 0.75\n",
    "\n",
    "split_threshold = int(len(data_batch) * TRAIN_SIZE)\n",
    "train_texts = data_batch[:split_threshold]\n",
    "test_texts = data_batch[split_threshold:]\n",
    "\n",
    "print('Traning size: {} | Validating size: {}'.format(len(train_texts), len(test_texts)))\n",
    "print('Total samples: {}'.format(len(train_texts) + len(test_texts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply BPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text(texts, out_file):\n",
    "    with open(out_file, 'w') as outf:\n",
    "        outf.write('\\n'.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = 'train_bpe.txt'\n",
    "bpe_model_name = \"story_bpe.yttm\"\n",
    "\n",
    "save_text(train_texts, train_txt)\n",
    "yttm.BPE.train(data=train_txt, vocab_size=1000, model=bpe_model_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = yttm.BPE(bpe_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "П16 Всем привет, вчера в Твери произошёл пиздец со стрельбой. Началось всё с того, что челики сцепились из за своих шкур они не поделили счёт в баре, челики пиздились и по итогу один ушёл отпизженый. Как оказалось, он ушёл за двумя хачами с обрезами, а второй челик я потом его нашёл и выпил по пивку, обсуждая данный пиздец, уже уебашил домой. Челики с обрезами походили, посмотрели и просто постреляли по шаурмечной, которая стояла рядом и тоже уебашили. Затем приехали 4 машины ментов, повтыкали минут 20 на шаурмечную и уехали. За Тверь город пиздец.\n",
      "\n",
      "[402, 63, 241, 12, 190, 213, 875, 126, 352, 137, 126, 299, 287, 154, 951, 235, 926, 513, 215, 273, 153, 238, 23, 230, 27, 443, 26, 377, 175, 713, 124, 717, 20, 158, 492, 143, 180, 124, 592, 223, 701, 243, 156, 335, 989, 225, 187, 13, 395, 150, 130, 200, 143, 143, 124, 26, 637, 126, 272, 153, 20, 492, 143, 180, 319, 170, 701, 127, 130, 771, 431, 155, 926, 178, 617, 32, 147, 28, 539, 560, 128, 233, 377, 405, 189, 155, 926, 156, 131, 228, 17, 21, 612, 26, 501, 124, 248, 153, 22, 501, 20, 181, 997, 576, 492, 143, 14, 176, 389, 229, 428, 675, 127, 209, 855, 130, 239, 15, 463, 248, 527, 32, 171, 21, 283, 10, 353, 513, 20, 355, 155, 212, 6, 658, 837, 27, 542, 7, 143, 180, 124, 248, 153, 22, 501, 130, 627, 476, 303, 533, 153, 143, 127, 537, 461, 153, 253, 143, 130, 572, 16, 13, 421, 679, 298, 818, 454, 21, 148, 4, 336, 426, 127, 567, 155, 212, 6, 249, 825, 614, 9, 190, 213, 478, 191, 649, 820, 199, 251, 779, 20, 130, 15, 342, 14, 191, 552, 173, 9, 416, 134, 572, 16, 13, 421, 26, 676, 127, 155, 478, 191, 27, 614, 299, 918, 740, 18, 513, 27]\n"
     ]
    }
   ],
   "source": [
    "random_id = np.random.randint(1, len(train_texts)-1)\n",
    "\n",
    "print(train_texts[random_id])\n",
    "print(\"\")\n",
    "print(tokenizer.encode(train_texts[random_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_ids = tokenizer.encode(train_texts, bos=True, eos=True)\n",
    "test_token_ids = tokenizer.encode(test_texts, bos=True, eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown n-grams in validation set:  29\n"
     ]
    }
   ],
   "source": [
    "unknown_subwords_in_test = sum(1 for text in test_token_ids for token_id in text if token_id == 1)\n",
    "print('Unknown n-grams in validation set: ', unknown_subwords_in_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data-loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(data, padding_value=0, batch_size=100, shuffle=True):\n",
    "    input_seq = []\n",
    "    target_seq = []\n",
    "    \n",
    "    for story in data:\n",
    "        input_seq.append(torch.tensor(story[:-1]))\n",
    "        target_seq.append(torch.tensor(story[1:]))\n",
    "    \n",
    "    input_seq = pad_sequence(input_seq, batch_first=True, padding_value=padding_value)\n",
    "    target_seq = pad_sequence(target_seq, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    data = torch.utils.data.TensorDataset(input_seq, target_seq)\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_loaders(train_token_ids)\n",
    "test_loader = get_loaders(test_token_ids, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Chp0UCPDDI3F"
   },
   "source": [
    "## Init Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_tools import dependency_mask, positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, backbone, emb_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "        self.backbone = backbone\n",
    "        self.out = nn.Linear(embedding_size, vocab_size)\n",
    "    \n",
    "    def forward(self, seed_token_ids):\n",
    "\n",
    "        batch_size, max_in_length = seed_token_ids.shape\n",
    "\n",
    "        seed_padding_mask = seed_token_ids == 0\n",
    "        dep_mask = dependency_mask(max_in_length).to(seed_token_ids.device)\n",
    "        \n",
    "        seed_embs = self.embeddings(seed_token_ids)  \n",
    "        pos_codes = positional_encoding(max_in_length,\n",
    "                                             self.embedding_size).unsqueeze(0).to(seed_embs.device)\n",
    "        seed_embs = seed_embs + pos_codes\n",
    "        seed_embs = self.emb_dropout(seed_embs)\n",
    "\n",
    "        \n",
    "        target_features = seed_embs\n",
    "        target_features = self.backbone(seed_embs,\n",
    "                                        mask=dep_mask,\n",
    "                                        src_key_padding_mask=seed_padding_mask)\n",
    "        \n",
    "        logits = self.out(target_features)  \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.impl = nn.TransformerEncoder(*args, **kwargs)\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def forward(self, src, *args, **kwargs):\n",
    "        src = src.transpose(0, 1).contiguous()  \n",
    "        result = self.impl(src, *args, **kwargs)  \n",
    "        result = result.transpose(0, 1).contiguous()  \n",
    "        return result\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for param in self.impl.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 2094312\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size()\n",
    "embedding_size = 256\n",
    "\n",
    "enoder = TransformerEncoder(nn.TransformerEncoderLayer(d_model=256, nhead=16, \n",
    "                                                              dim_feedforward=512, dropout=0.4), num_layers=3)\n",
    "\n",
    "\n",
    "model = LanguageModel(vocab_size, embedding_size, enoder, emb_dropout=0.1)\n",
    "print('Params:', sum(t.numel() for t in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-3\n",
    "EPOCH = 50\n",
    "reg_alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=reg_alpha)\n",
    "torch_transf_model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_tools import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0m 28s | Epoch: 1 / 50 | T-Loss: 6.290 | Val-Loss: 6.191\n",
      "Time: 0m 56s | Epoch: 2 / 50 | T-Loss: 6.168 | Val-Loss: 6.073\n",
      "Time: 1m 23s | Epoch: 3 / 50 | T-Loss: 5.993 | Val-Loss: 5.762\n",
      "Time: 1m 51s | Epoch: 4 / 50 | T-Loss: 5.587 | Val-Loss: 5.193\n",
      "Time: 2m 18s | Epoch: 5 / 50 | T-Loss: 4.908 | Val-Loss: 4.652\n",
      "Time: 2m 46s | Epoch: 6 / 50 | T-Loss: 4.500 | Val-Loss: 4.540\n",
      "Time: 3m 13s | Epoch: 7 / 50 | T-Loss: 4.327 | Val-Loss: 4.481\n",
      "Time: 3m 41s | Epoch: 8 / 50 | T-Loss: 4.210 | Val-Loss: 4.455\n",
      "Time: 4m 8s | Epoch: 9 / 50 | T-Loss: 4.109 | Val-Loss: 4.431\n",
      "Time: 4m 36s | Epoch: 10 / 50 | T-Loss: 4.027 | Val-Loss: 4.417\n",
      "Time: 5m 3s | Epoch: 11 / 50 | T-Loss: 3.949 | Val-Loss: 4.408\n",
      "Time: 5m 31s | Epoch: 12 / 50 | T-Loss: 3.884 | Val-Loss: 4.402\n",
      "Time: 5m 58s | Epoch: 13 / 50 | T-Loss: 3.825 | Val-Loss: 4.396\n",
      "Time: 6m 26s | Epoch: 14 / 50 | T-Loss: 3.769 | Val-Loss: 4.397\n",
      "Time: 6m 53s | Epoch: 15 / 50 | T-Loss: 3.718 | Val-Loss: 4.396\n",
      "Time: 7m 21s | Epoch: 16 / 50 | T-Loss: 3.678 | Val-Loss: 4.395\n",
      "Time: 7m 48s | Epoch: 17 / 50 | T-Loss: 3.632 | Val-Loss: 4.405\n",
      "Time: 8m 16s | Epoch: 18 / 50 | T-Loss: 3.596 | Val-Loss: 4.413\n",
      "Time: 8m 43s | Epoch: 19 / 50 | T-Loss: 3.561 | Val-Loss: 4.409\n",
      "Time: 9m 11s | Epoch: 20 / 50 | T-Loss: 3.525 | Val-Loss: 4.419\n",
      "Time: 9m 39s | Epoch: 21 / 50 | T-Loss: 3.495 | Val-Loss: 4.424\n",
      "Time: 10m 6s | Epoch: 22 / 50 | T-Loss: 3.465 | Val-Loss: 4.429\n",
      "Time: 10m 34s | Epoch: 23 / 50 | T-Loss: 3.439 | Val-Loss: 4.431\n",
      "Time: 11m 1s | Epoch: 24 / 50 | T-Loss: 3.412 | Val-Loss: 4.442\n",
      "Time: 11m 29s | Epoch: 25 / 50 | T-Loss: 3.391 | Val-Loss: 4.447\n",
      "Time: 11m 56s | Epoch: 26 / 50 | T-Loss: 3.367 | Val-Loss: 4.446\n",
      "Time: 12m 24s | Epoch: 27 / 50 | T-Loss: 3.346 | Val-Loss: 4.460\n",
      "Time: 12m 51s | Epoch: 28 / 50 | T-Loss: 3.323 | Val-Loss: 4.466\n",
      "Time: 13m 19s | Epoch: 29 / 50 | T-Loss: 3.307 | Val-Loss: 4.465\n",
      "Time: 13m 46s | Epoch: 30 / 50 | T-Loss: 3.285 | Val-Loss: 4.471\n",
      "Time: 14m 14s | Epoch: 31 / 50 | T-Loss: 3.268 | Val-Loss: 4.487\n",
      "Time: 14m 41s | Epoch: 32 / 50 | T-Loss: 3.253 | Val-Loss: 4.483\n",
      "Time: 15m 9s | Epoch: 33 / 50 | T-Loss: 3.239 | Val-Loss: 4.492\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Time: 15m 36s | Epoch: 34 / 50 | T-Loss: 3.224 | Val-Loss: 4.502\n",
      "Time: 16m 4s | Epoch: 35 / 50 | T-Loss: 3.146 | Val-Loss: 4.495\n",
      "Time: 16m 31s | Epoch: 36 / 50 | T-Loss: 3.099 | Val-Loss: 4.507\n",
      "Time: 16m 59s | Epoch: 37 / 50 | T-Loss: 3.081 | Val-Loss: 4.518\n",
      "Time: 17m 26s | Epoch: 38 / 50 | T-Loss: 3.067 | Val-Loss: 4.531\n",
      "Time: 17m 54s | Epoch: 39 / 50 | T-Loss: 3.055 | Val-Loss: 4.545\n",
      "Time: 18m 21s | Epoch: 40 / 50 | T-Loss: 3.046 | Val-Loss: 4.545\n",
      "Time: 18m 49s | Epoch: 41 / 50 | T-Loss: 3.037 | Val-Loss: 4.549\n",
      "Time: 19m 17s | Epoch: 42 / 50 | T-Loss: 3.025 | Val-Loss: 4.556\n",
      "Time: 19m 44s | Epoch: 43 / 50 | T-Loss: 3.017 | Val-Loss: 4.556\n",
      "Time: 20m 12s | Epoch: 44 / 50 | T-Loss: 3.007 | Val-Loss: 4.563\n",
      "Time: 20m 39s | Epoch: 45 / 50 | T-Loss: 3.002 | Val-Loss: 4.573\n",
      "Time: 21m 7s | Epoch: 46 / 50 | T-Loss: 2.995 | Val-Loss: 4.570\n",
      "Time: 21m 34s | Epoch: 47 / 50 | T-Loss: 2.989 | Val-Loss: 4.577\n",
      "Time: 22m 2s | Epoch: 48 / 50 | T-Loss: 2.984 | Val-Loss: 4.584\n",
      "Time: 22m 29s | Epoch: 49 / 50 | T-Loss: 2.973 | Val-Loss: 4.575\n",
      "Time: 22m 57s | Epoch: 50 / 50 | T-Loss: 2.968 | Val-Loss: 4.588\n"
     ]
    }
   ],
   "source": [
    "torch_transf_model = train_loop(model, device,optimizer, train_loader, test_loader, epoch_value = EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0splSYRDI3G"
   },
   "source": [
    "## Fit model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text(model, tokenizer, seq_begin):\n",
    "    eos_token = 3\n",
    "    max_steps_n = 40\n",
    "    \n",
    "    seed_tokens = tokenizer.encode([seq_begin])[0]\n",
    "    \n",
    "    for _ in range(max_steps_n):\n",
    "        in_batch = torch.tensor(seed_tokens).unsqueeze(0).to(device)\n",
    "        best_next_token = model(in_batch)[0, -1].argmax()\n",
    "        if best_next_token == eos_token:\n",
    "            break\n",
    "\n",
    "        seed_tokens.append(best_next_token)\n",
    "\n",
    "    return tokenizer.decode([seed_tokens])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Школа это скучно. История такая история, она была в школе, она была однажды она мне ее парню и она сидит на хате. И вот она сидит на хате перед'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_text(model, tokenizer, \"Школа это скучно\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lm_sent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
