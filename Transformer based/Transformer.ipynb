{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HzETJPTeDI2l"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import youtokentome as yttm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaS0OXNbDI2v"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 21 11:33:50 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4swSGSVDI25"
   },
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTSxoD8cDI25"
   },
   "outputs": [],
   "source": [
    "story_path = \"corpus/story_data_punct_del_em.pkl\"\n",
    "with open(story_path, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iNrocaqPSifM"
   },
   "outputs": [],
   "source": [
    "STORY_VAL = 34000\n",
    "data_batch = data[:STORY_VAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples2text(data, story_size=300):\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        text = \" \".join(data[i]).replace(\" .\",\".\").replace(\" ,\", \",\").replace(\" ?\", \"?\")\n",
    "        \n",
    "        if len(text) <= story_size:\n",
    "            new_data.append(text)\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch = list(samples2text(data_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Привет, Позор ! Сегодня захавал 2 пачки дошика, пес поел тушенки из фикс прайса. Щас лежим в одной комнате, пропердели все так, что если зажечь зажигалку, то воздух по всей комнате воспламенится. Мне страшно.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_story = np.random.randint(0, len(data_batch)-1)\n",
    "data_batch[random_story]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_tools import split_data, get_bpe_tokenizer, get_unknown_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 21061\n",
      "\n",
      "Traning size: 15795 | Validating size: 5266\n"
     ]
    }
   ],
   "source": [
    "train_texts, test_texts = split_data(data_batch, train_size=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply BPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = 'train_bpe.txt'\n",
    "bpe_model_name = \"story_bpe.yttm\"\n",
    "BPE_VOCAB_SIZE = 1000\n",
    "\n",
    "tokenizer = get_bpe_tokenizer(train_texts, train_txt_path=train_txt, bpe_model_name=bpe_model_name,\n",
    "                              vocab_size=BPE_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "К словам физика Вот например, пойдёшь в 18 лет в армию, на войну, поубиваешь там всех, а потом в магазин придешь за водкой и тебе скажут мальчик тебе сколько лет? Водка с 21..\n",
      "\n",
      "215 877 17 911 8 163 227 207 152 19 172 830 20 753 18 37 321 144 249 66 328 144 982 281 404 152 144 240 191 20 147 16 394 187 487 395 330 724 195 450 144 372 289 581 10 245 255 321 175 277 18 346 145 547 7 141 163 366 9 151 350 304 14 547 7 883 464 328 50 227 5 18 163 141 283 39 258\n"
     ]
    }
   ],
   "source": [
    "random_id = np.random.randint(1, len(train_texts)-1)\n",
    "\n",
    "print(train_texts[random_id])\n",
    "print(\"\")\n",
    "print(*tokenizer.encode(train_texts[random_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_ids = tokenizer.encode(train_texts, bos=True, eos=True)\n",
    "test_token_ids = tokenizer.encode(test_texts, bos=True, eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown n-grams in validation set:  0\n"
     ]
    }
   ],
   "source": [
    "get_unknown_ngrams(test_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data-loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(data, padding_value=0, batch_size=512, shuffle=True):\n",
    "    input_seq = []\n",
    "    target_seq = []\n",
    "    \n",
    "    for story in data:\n",
    "        input_seq.append(torch.tensor(story[:-1]))\n",
    "        target_seq.append(torch.tensor(story[1:]))\n",
    "    \n",
    "    input_seq = pad_sequence(input_seq, batch_first=True, padding_value=padding_value)\n",
    "    target_seq = pad_sequence(target_seq, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    data = torch.utils.data.TensorDataset(input_seq, target_seq)\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_loaders(train_token_ids)\n",
    "test_loader = get_loaders(test_token_ids, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Chp0UCPDDI3F"
   },
   "source": [
    "## Init Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_tools import dependency_mask, positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, backbone, emb_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "        self.backbone = backbone\n",
    "        self.out = nn.Linear(embedding_size, vocab_size)\n",
    "    \n",
    "    def forward(self, seed_token_ids):\n",
    "\n",
    "        batch_size, max_in_length = seed_token_ids.shape\n",
    "\n",
    "        seed_padding_mask = seed_token_ids == 0\n",
    "        dep_mask = dependency_mask(max_in_length).to(seed_token_ids.device)\n",
    "        \n",
    "        seed_embs = self.embeddings(seed_token_ids)  \n",
    "        pos_codes = positional_encoding(max_in_length,\n",
    "                                             self.embedding_size).unsqueeze(0).to(seed_embs.device)\n",
    "        seed_embs = seed_embs + pos_codes\n",
    "        seed_embs = self.emb_dropout(seed_embs)\n",
    "\n",
    "        \n",
    "        target_features = seed_embs\n",
    "        target_features = self.backbone(seed_embs,\n",
    "                                        mask=dep_mask,\n",
    "                                        src_key_padding_mask=seed_padding_mask)\n",
    "        \n",
    "        logits = self.out(target_features)  \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=16, \n",
    "                                                        dim_feedforward=512, dropout=0.3)\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, src, *args, **kwargs):\n",
    "        # read dep_mask and src_key_padding_mask\n",
    "        # Transpose seq to Batch First\n",
    "        \n",
    "        src = src.transpose(0, 1).contiguous()\n",
    "        \n",
    "        results = self.encoder(src, *args, **kwargs)\n",
    "        \n",
    "        results = results.transpose(0, 1).contiguous()\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def init_weights(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 2094312\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size()\n",
    "embedding_size = 256\n",
    "\n",
    "model = LanguageModel(vocab_size, embedding_size, Transformer(), emb_dropout=0.1)\n",
    "print('Params:', sum(t.numel() for t in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (embeddings): Embedding(1000, 256, padding_idx=0)\n",
       "  (emb_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (backbone): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=256, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-3\n",
    "EPOCH = 40\n",
    "reg_alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=reg_alpha)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_tools import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0m 30s | Epoch: 1 / 10 | T-Loss: 3.057 | Val-Loss: 2.772\n",
      "Time: 1m 0s | Epoch: 2 / 10 | T-Loss: 3.054 | Val-Loss: 2.774\n",
      "Time: 1m 31s | Epoch: 3 / 10 | T-Loss: 3.049 | Val-Loss: 2.770\n",
      "Time: 2m 1s | Epoch: 4 / 10 | T-Loss: 3.041 | Val-Loss: 2.760\n",
      "Time: 2m 32s | Epoch: 5 / 10 | T-Loss: 3.037 | Val-Loss: 2.755\n",
      "Time: 3m 2s | Epoch: 6 / 10 | T-Loss: 3.034 | Val-Loss: 2.752\n",
      "Time: 3m 33s | Epoch: 7 / 10 | T-Loss: 3.030 | Val-Loss: 2.743\n",
      "Time: 4m 4s | Epoch: 8 / 10 | T-Loss: 3.024 | Val-Loss: 2.734\n",
      "Time: 4m 34s | Epoch: 9 / 10 | T-Loss: 3.020 | Val-Loss: 2.733\n",
      "Time: 5m 5s | Epoch: 10 / 10 | T-Loss: 3.015 | Val-Loss: 2.732\n"
     ]
    }
   ],
   "source": [
    "model = train_loop(model, device, optimizer, train_loader, test_loader, epoch_value=10, plot_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Epoch: 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0splSYRDI3G"
   },
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_tools import create_greedy_text, BeamGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Я пошел в магазин утром в магазине. В итоге все слышу какие то мужики и вижу как мужик смотрит нам, что он говорит, что это может это????'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_greedy_text(model, tokenizer, \"Я пошел в магазин утром\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_generator = BeamGenerator(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "3.934856298521477\n",
      "Я пошел в магазин утром был в магазин. Так вот, сегодня спросил, что он ответил, что на грани фантастики.<EOS>\n",
      "\n",
      "--------------------\n",
      "4.0701849869727855\n",
      "Я пошел в магазин утром был в магазин. Так вот, сегодня спросил, что он ответил что на грани фантастики.<EOS>\n",
      "\n",
      "--------------------\n",
      "4.201108203673812\n",
      "Я пошел в магазин утром был в магазин. Так вот, сегодня спросил, что он ответил, что на грани фантастики<EOS>\n",
      "\n",
      "--------------------\n",
      "4.324112428351587\n",
      "Я пошел в магазин утром был в магазин. Так вот, сегодня спросил, что он ответил, что на грани фантастики !<EOS>\n",
      "\n",
      "--------------------\n",
      "4.398650195709798\n",
      "Я пошел в магазин утром был в магазин. Так вот, сегодня спросил, что он ответил что на грани фантастики<EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beam_gen_variants = beam_generator('Я пошел в магазин утром',100, beamsize=5, return_hypotheses_n=5)\n",
    "\n",
    "for score, pred_txt in beam_gen_variants:\n",
    "    print('--------------------')\n",
    "    print(score)\n",
    "    print(pred_txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': 3.203}, 'story_model_trs.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer and PyTorch model(for Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_model_name = \"story_bpe.yttm\"\n",
    "tokenizer = yttm.BPE(bpe_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size()\n",
    "embedding_size = 256\n",
    "\n",
    "model = LanguageModel(vocab_size, embedding_size, Transformer(), emb_dropout=0.1)\n",
    "\n",
    "LR = 2e-3\n",
    "EPOCH = 40\n",
    "reg_alpha = 0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=reg_alpha)\n",
    "\n",
    "checkpoint = torch.load(story_model_fldct.pth)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lm_sent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
